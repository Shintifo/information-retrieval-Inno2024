{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Submission Instructions and Format\n",
    "\n",
    "### 1. **Naming Convention for Submission**\n",
    "- Each student must submit their homework as a Jupyter Notebook (`.ipynb`) file.\n",
    "- The file must be named with your **email address**. For example, if your email is `j.doe@innopolis.university`, the file should be named `j.doe@innopolis.university.ipynb`.\n",
    "- Make sure that the notebook contains all necessary classes and functions outlined in the tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Structure of the Homework**\n",
    "\n",
    "The homework is divided into **four tasks**, and each task requires you to implement specific classes and functions. The tasks build upon concepts of web scraping, HTML parsing, document processing, and crawling.\n",
    "\n",
    "Hereâ€™s a breakdown of what is expected in each task:\n",
    "\n",
    "#### **Task 1: Artifact Caching System**\n",
    "- **Class**: `Artifact`\n",
    "  - Implement a class that can **download**, **store**, and **retrieve** digital content from a URL.\n",
    "  - This class should:\n",
    "    - Fetch content from a URL and store it in memory.\n",
    "    - Save the content to a local file to avoid redundant downloads.\n",
    "    - Retrieve the content from the local cache if it already exists.\n",
    "- **Methods**:\n",
    "  - `fetch_artifact()`: Downloads the content from the URL.\n",
    "  - `store_artifact()`: Stores the content locally in a unique file.\n",
    "  - `retrieve_artifact()`: Retrieves the content from the local cache.\n",
    "  \n",
    "---\n",
    "\n",
    "#### **Task 2: Smithsonian Snapshot Parser**\n",
    "- **Class**: `SmithsonianParser`\n",
    "  - Implement a class that parses HTML pages like [Smithsonian Snapshots](https://www.si.edu/newsdesk/snapshot/how-very-logical).\n",
    "  - This class should:\n",
    "    - Extract all links (`<a>` tags) and store them as a list of tuples.\n",
    "    - Extract all image URLs (`<img>` tags) and store them in a list.\n",
    "    - Clean the text from the page, removing unnecessary elements (scripts, styles).\n",
    "- **Methods**:\n",
    "  - `fetch_page()`: Downloads the HTML content of a page.\n",
    "  - `parse()`: Parses the content and extracts links, images, and cleaned text.\n",
    "  - `get_anchors()`, `get_images()`, and `get_text()`: Returns the extracted data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Task 3: Text Analysis of Smithsonian Snapshot**\n",
    "- **Class**: `SmithsonianTextAnalyzer`\n",
    "  - Implement a class that analyzes the text content of a Smithsonian Snapshot page.\n",
    "  - This class should:\n",
    "    - Perform **word frequency analysis**.\n",
    "    - **Segment sentences** and split text properly.\n",
    "    - Clean the text to remove special characters and whitespace.\n",
    "- **Methods**:\n",
    "  - `analyze()`: Fetches the page content, processes the text, and generates word frequency statistics.\n",
    "  - `get_word_stats()`: Returns word frequency in the form of a `Counter` object.\n",
    "  - `split_into_sentences()`: Splits the text into sentences.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Task 4: Smithsonian Snapshot Web Crawler**\n",
    "- **Class**: `SmithsonianCrawler`\n",
    "  - Implement a web crawler that starts at the [Smithsonian Snapshots](https://www.si.edu/newsdesk/snapshots) page and crawls through linked snapshot articles.\n",
    "  - This class should:\n",
    "    - Crawl pages to a specified depth.\n",
    "    - Extract links, images, and cleaned text from each page.\n",
    "    - Return the results as soon as the page is processed.\n",
    "- **Methods**:\n",
    "  - `crawl()`: Recursively visits pages starting from a given URL.\n",
    "  - `crawl_generator()`: Generates content as the crawler processes each page.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Grading Process**\n",
    "- Each homework will be graded using an automated grading system.\n",
    "- The grading system will dynamically import and execute your code to test if all the tasks are implemented correctly.\n",
    "  \n",
    "---\n",
    "\n",
    "### 4. **Total Grade Breakdown**\n",
    "- **Task 1**: Artifact Caching System (25 points)\n",
    "- **Task 2**: Smithsonian Snapshot Parser (25 points)\n",
    "- **Task 3**: Text Analysis (25 points)\n",
    "- **Task 4**: Web Crawler (25 points)\n",
    "- **Total**: 100 points\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Detailed Feedback**\n",
    "- Feedback will be provided with specific details on:\n",
    "  - **What worked**: Indicating which parts of the code were implemented correctly and passed the tests.\n",
    "  - **What needs improvement**: Highlighting which tests failed and what parts of the code may require debugging or further development.\n",
    "- The feedback will help you understand your performance in each task.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Submission Guidelines**\n",
    "- Ensure that your notebook is properly formatted and runs without errors.\n",
    "- Do not use any external libraries unless instructed.\n",
    "- Each function and class must follow the naming conventions provided in this document.\n",
    "- Submit your notebook on time. Late submissions may not be accepted.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Final Tips**\n",
    "- Test each task thoroughly before submission.\n",
    "- Ensure your notebook is readable and well-documented.\n",
    "- Make use of comments to explain your code wherever necessary.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Archiving Virtual Artifacts - Preserving a Digital Museum\n",
    "\n",
    "#### 1.0.1. Task Description\n",
    "Imagine you are a data archivist working to preserve artifacts from the **Smithsonian Institution's digital collection**. Your job is to download, store, and manage different types of data (such as images, videos, and documents) to ensure they can be accessed later without repeated downloads.\n",
    "\n",
    "Use the [Smithsonian Institution Collections](https://www.si.edu/snapshot) as your source of artifacts. You are tasked with building a caching system that can store downloaded files in a structured way and retrieve them as needed.\n",
    "\n",
    "#### Tasks:\n",
    "1. `fetch_artifact() -> bool`: Download content from the Smithsonian's collection page based on a provided URL. The method should return `True` if the download is successful, or `False` if the download fails. This method will handle errors like network issues and non-existent pages gracefully.\n",
    "   \n",
    "2. `store_artifact(directory: str = \"artifact_cache\") -> None`: Save the content of the artifact (text, image, etc.) in a local file system. Each artifact must be stored in its own unique file based on its URL. If the content is already present in the directory, it should not overwrite existing files.\n",
    "\n",
    "3. `retrieve_artifact(directory: str = \"artifact_cache\") -> bool`: Load an artifact from local storage using its URL. This ensures that the content is cached correctly and avoids redundant downloads by checking the local file system before attempting to download again.\n",
    "\n",
    "#### Criteria for Success:\n",
    "- Different URLs must map to different files, even if they belong to the same domain. This can be achieved by generating a unique filename using a hash function (e.g., `md5`).\n",
    "- Binary files (e.g., images) must be handled correctly without corruption. This requires proper file handling, ensuring binary write operations for non-text artifacts.\n",
    "- Artifacts that are already stored locally should not be downloaded again. The caching system should check for the existence of a file before re-downloading it.\n",
    "\n",
    "#### Link: [Smithsonian Institution Collections](https://www.si.edu/newsdesk/snapshot/what-good-boy)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T16:15:31.050547Z",
     "start_time": "2024-09-17T16:15:30.945825Z"
    }
   },
   "source": [
    "from typing import Optional\n",
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class Artifact:\n",
    "\tdef __init__(self, url: str) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the Artifact with a URL.\n",
    "\t\t:param url: The URL of the artifact.\n",
    "\t\t\"\"\"\n",
    "\t\tself.url: str = url\n",
    "\t\tself.content: Optional[bytes] = None  # Content is None initially and becomes bytes if fetched\n",
    "\t\tself.filename: Optional[str] = None  # Filename is generated later\n",
    "\n",
    "\tdef generate_filename(self) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tGenerates a unique and safe filename based on the URL using a hash.\n",
    "\t\t:return: None. Updates the self.filename attribute.\n",
    "\t\t\"\"\"\n",
    "\t\tfilename = hashlib.sha512(self.url.encode('utf-8')).hexdigest() + \".bin\"\n",
    "\t\treturn filename\n",
    "\n",
    "\n",
    "\tdef fetch_artifact(self) -> bool:\n",
    "\t\t\"\"\"\n",
    "\t\tDownload the artifact from the given URL and store its content in memory.\n",
    "\t\t:return: True if the download is successful, False otherwise.\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(self.url)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\t\t\tself.content = response.content\n",
    "\t\t\tself.filename = self.generate_filename()\n",
    "\t\t\treturn True\n",
    "\t\texcept requests.exceptions.RequestException:\n",
    "\t\t\treturn False\n",
    "\t\n",
    "\t\n",
    "\tdef store_artifact(self, directory: str = \"artifact_cache\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tStore the artifact content in a local file in a cache directory.\n",
    "\t\t:param directory: Directory to store the artifact. Default is 'artifact_cache'.\n",
    "\t\t:return: None. Stores the file locally.\n",
    "\t\t\"\"\"\n",
    "\t\tif self.content is None:\n",
    "\t\t\treturn\n",
    "\t\n",
    "\t\tif not os.path.exists(directory):\n",
    "\t\t\tos.mkdir(directory)\n",
    "\t\n",
    "\t\tfilepath = os.path.join(directory, self.filename)\n",
    "\t\tif os.path.exists(filepath):\n",
    "\t\t\treturn\n",
    "\t\n",
    "\t\twith open(filepath, 'wb') as file:\n",
    "\t\t\tfile.write(bytes(self.content))\n",
    "\t\tprint(\"Artifact stored at \", filepath)\n",
    "\t\n",
    "\t\n",
    "\tdef retrieve_artifact(self, directory: str = \"artifact_cache\") -> bool:\n",
    "\t\t\"\"\"\n",
    "\t\tRetrieve the artifact from the local cache if it has been stored before.\n",
    "\t\t:param directory: Directory to look for the artifact. Default is 'artifact_cache'.\n",
    "\t\t:return: True if the artifact is successfully retrieved, False otherwise.\n",
    "\t\t\"\"\"\n",
    "\t\tfilepath = os.path.join(directory, self.filename)\n",
    "\t\tif os.path.exists(filepath):\n",
    "\t\t\twith open(filepath, 'rb') as file:\n",
    "\t\t\t\tself.content = file.read\n",
    "\t\t\tprint(\"Artifact loaded from \", filepath)\n",
    "\t\t\treturn True\n",
    "\t\treturn False"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Parsing Web Pages - Smithsonian Snapshot\n",
    "\n",
    "#### 2.0.2. Task Description\n",
    "For this task, you will be working with pages from the [Smithsonian Newsdesk Snapshot](https://www.si.edu/newsdesk/snapshot/how-very-logical). Your goal is to extract meaningful content such as links, images, and clean text from the page.\n",
    "\n",
    "You will need to:\n",
    "1. **Extract Hyperlinks (`get_anchors() -> List[Tuple[str, str]]`)**: Extract all anchor tags (`<a>`) from the page and store them as a list of tuples in the format `('link_text', 'absolute_url')`. Ensure that relative URLs are properly converted to absolute URLs using the page's base URL.\n",
    "   \n",
    "2. **Collect Image URLs (`get_images() -> List[str]`)**: Collect all image URLs (`<img>`) from the page and store them in a list. As with links, convert relative image URLs to absolute URLs for consistency and accessibility.\n",
    "   \n",
    "3. **Extract Clean Text (`get_text() -> str`)**: Extract the main plain text content from the page while removing unnecessary elements such as scripts, styles, and comments. The extracted text should be cleaned and made human-readable.\n",
    "\n",
    "#### Criteria for Success:\n",
    "- Extract all hyperlinks and store them as a list of tuples, ensuring that relative URLs are handled correctly by converting them into absolute URLs using the base URL.\n",
    "- Collect all image URLs, ensuring they are stored as absolute URLs.\n",
    "- The extracted text should be free of scripts, styles, and other non-content elements and should represent the clean, readable main content of the page.\n",
    "\n",
    "#### Link: [How Very Logical](https://www.si.edu/newsdesk/snapshot/how-very-logical)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T16:27:45.901338Z",
     "start_time": "2024-09-17T16:27:45.894546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Tuple, Optional\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SmithsonianParser:\n",
    "\tdef __init__(self, url: str) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the SmithsonianParser with a URL.\n",
    "\t\t:param url: The URL of the page to be parsed.\n",
    "\t\t\"\"\"\n",
    "\t\tself.url: str = url\n",
    "\t\tself.anchors: List[Tuple[str, str]] = []  # List of tuples (link_text, absolute_url)\n",
    "\t\tself.images: List[str] = []  # List of image URLs\n",
    "\t\tself.text: str = \"\"  # Cleaned text content\n",
    "\n",
    "\tdef fetch_page(self) -> Optional[bytes]:\n",
    "\t\t\"\"\"\n",
    "\t\tFetch the HTML content of the given URL.\n",
    "\t\t:return: The page content as bytes if successful, otherwise None.\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(self.url)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\t\t\treturn response.content\n",
    "\t\texcept requests.exceptions.RequestException:\n",
    "\t\t\treturn None\n",
    "\n",
    "\tdef parse(self, html_content: bytes) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tParse the HTML content using BeautifulSoup.\n",
    "\t\t1. Extract all anchor tags and store them as ('link_text', 'absolute_url').\n",
    "\t\t2. Extract all image URLs and store them in a list.\n",
    "\t\t3. Extract clean, readable text from the page.\n",
    "\t\t:param html_content: The HTML content of the page to be parsed.\n",
    "\t\t:return: None.\n",
    "\t\t\"\"\"\n",
    "\t\tsoup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\t\tfor link in soup.find_all('a', href=True):\n",
    "\t\t\tabs_url = urljoin(self.url, link['href'])\n",
    "\t\t\ttext = link.text\n",
    "\t\t\ttext = re.sub(r'\\s+', ' ', text).strip()\n",
    "\t\t\tself.anchors.append((text, abs_url))\n",
    "\n",
    "\t\tfor img in soup.find_all('img', src=True):\n",
    "\t\t\tabsolute_url = urljoin(self.url, img['src'])\n",
    "\t\t\tself.images.append(absolute_url)\n",
    "\t\t\n",
    "\t\textra_tags = ['style', 'script', 'head', 'title', 'meta', '[document]']\n",
    "\t\tfor tag in extra_tags:\n",
    "\t\t\tfor element in soup.find_all(tag):\n",
    "\t\t\t\telement.decompose()\n",
    "\t\t\n",
    "\t\ttext = soup.get_text(separator=\" \")\n",
    "\t\t\n",
    "\t\ttext = re.sub(r'\\s+', ' ', text)\n",
    "\t\tself.text = text.strip()\n",
    "\t\tprint(text)\n",
    "\n",
    "\tdef get_anchors(self) -> List[Tuple[str, str]]:\n",
    "\t\t\"\"\"\n",
    "\t\tReturn the list of anchors extracted from the page.\n",
    "\t\t:return: A list of tuples with link text and absolute URL.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.anchors\n",
    "\n",
    "\tdef get_images(self) -> List[str]:\n",
    "\t\t\"\"\"\n",
    "\t\tReturn the list of image URLs extracted from the page.\n",
    "\t\t:return: A list of image URLs.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.images\n",
    "\n",
    "\tdef get_text(self) -> str:\n",
    "\t\t\"\"\"\n",
    "\t\tReturn the cleaned text content extracted from the page.\n",
    "\t\t:return: The extracted text content as a string.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.text"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T16:27:47.303974Z",
     "start_time": "2024-09-17T16:27:46.511707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cll = SmithsonianParser(\"https://www.si.edu/newsdesk/snapshot/how-very-logical\")\n",
    "cll.parse(cll.fetch_page())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skip to main content Search Search What is 2+5? My Visit Donate Smithsonian Institution Site Navigation Visit Hours and Locations Entry and Guidelines Maps and Brochures Dining and Shopping Accessibility Visiting with Kids Group Visits Group Sales What's On Exhibitions Current Upcoming Past Today's Events Online Events All Events IMAX & Planetarium Explore - Art & Design - History & Culture - Science & Nature Collections Open Access Research Resources Libraries Archives Smithsonian Institution Archives Air and Space Museum Anacostia Community Museum American Art Museum Archives of American Art Archives of American Gardens American History Museum American Indian Museum Asian Art Museum Archives Eliot Elisofon Photographic Archives, African Art Hirshhorn Archive National Anthropological Archives National Portrait Gallery Ralph Rinzler Archives, Folklife Libraries' Special Collections Podcasts Mobile Apps Learn For Caregivers For Educators Art & Design Resources Science & Nature Resources Social Studies & Civics Resources Professional Development Events for Educators Field Trips For Students For Academics For Lifelong Learners Support Us Become a Member Renew Membership Make a Gift Volunteer Behind-the-Scenes Digital Volunteers Museum Information Desk Smithsonian Call Center Docent Programs Participatory Science About Our Organization Board of Regents Members Committees Reading Room Bylaws, Policies and Procedures Schedules and Agendas Meeting Minutes Actions Webcasts Contact Museums and Zoo Research Centers Cultural Centers Education Centers General Counsel Legal History Internships Records Requests Reading Room Tort Claim Subpoenas & Testimonies Events Office of Human Resources Employee Benefits How to Apply Job Opportunities Job Seekers with Disabilities Frequently Asked Questions SI Civil Program Contact Us Equal Employment Office EEO Complaint Process Individuals with Disabilities Special Emphasis Program Supplier Diversity Program Doing Business with Us Policies and Procedures Additional Resources Sponsored Projects Policies Combating Trafficking in Persons Animal Care and Use Human Research Reports Internships Our Leadership Reports and Plans Annual Reports Metrics Dashboard Dashboard Home Virtual Smithsonian Public Engagement National Collections Research People & Operations One Smithsonian Strategic Plan Newsdesk News Releases Media Contacts Photos and Video Media Kits Fact Sheets Visitor Stats Secretary and Admin Bios Filming Requests Smithsonian Snapshot A peek into our collections, one object at a time How Very Logical October 17, 2023 Original Mr. Spock ear tips (A20220161000). Smithsonian photo by Eric Long. Pictured above are foam pointed ear tips that Leonard Nimoy wore in his role as Spock in the original 1960s Star Trek TV series, which aired on NBC from 1966 until 1969. Nimoy brought the prosthetics home as a personal memento of his role as the half-human, half-Vulcan science officer on the starship Enterprise. Until his death in 2015, the ear tips were displayed in Nimoyâ€™s home in a box he made. Donated to the Smithsonianâ€™s National Air and Space Museum by Nimoyâ€™s children Adam and Julie and the Nimoy family, they are now on display in the museumâ€™s Kenneth C. Griffin Exploring the Planets Gallery. Watch a video of the unboxing of the ear tips , read more about them , and view the museumâ€™s Enterprise model that was also used in the original series. The ear tips were a gift of the Nimoy Family, in honor of Beit T'Shuvah and the Leonard Nimoy COPD Research Fund at UCLA. Collections Open Access Smithsonian Snapshot A Witness to War In a Civil War battle, bullets cut down an oak tree. More Smithsonian Snapshots Â» Footer logo Link to homepage Footer navigation Contact Us Job Opportunities Get Involved Inspector General Records Requests Accessibility EEO & Supplier Diversity Shop Online Host Your Event Press Room Privacy Terms of Use Social media links Facebook Instagram YouTube LinkedIn Get the latest news from the Smithsonian Sign up for Smithsonian e-news Get the latest news from the Smithsonian Sign up for Smithsonian e-news: * Email powered by BlackBaud (Privacy Policy, Terms of Use) CAPTCHA This question is for testing whether or not you are a human visitor and to prevent automated spam submissions. Back to Top \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 3: Summarizing Smithsonian Snapshots\n",
    "\n",
    "#### 3.0.3. Task Description\n",
    "You will analyze the text content from one of the Smithsonian Snapshot pages, such as [How Very Logical](https://www.si.edu/newsdesk/snapshot/how-very-logical). Your task is to extract and analyze the text content from this page, focusing on the following:\n",
    "\n",
    "1. **Extract key phrases (`get_word_stats() -> Counter`)**: Use basic natural language processing (NLP) techniques to identify the most frequently used words from the main body of the text. All words should be converted to lowercase, and a frequency distribution of the words should be generated using a `Counter` object.\n",
    "   \n",
    "2. **Sentence Segmentation (`split_into_sentences() -> None`)**: Split the cleaned text into individual sentences, ensuring that punctuation is handled correctly. The sentences should be stored as a list of strings.\n",
    "\n",
    "3. **Cleaning and Normalization (`clean_text(html_content: bytes) -> None`)**: Clean the text to remove any extraneous characters, symbols, or whitespace. This involves normalizing the text by converting it to lowercase and removing special characters like punctuation.\n",
    "\n",
    "### Tasks:\n",
    "1. **Word Frequency Analysis**: Implement a method (`get_word_stats()`) to count the frequency of each word in the page content. Ensure all words are converted to lowercase for consistency in counting, and return the word frequencies as a `Counter` object.\n",
    "   \n",
    "2. **Sentence Splitting**: Implement a method (`split_into_sentences()`) to split the content into individual sentences, ensuring punctuation and line breaks are handled properly. The resulting sentences should be stored in a list.\n",
    "   \n",
    "3. **Cleaning and Normalization**: Implement a method (`clean_text(html_content: bytes)`) to clean the text by removing any special characters and unnecessary whitespace, while also normalizing the text by converting it to lowercase.\n",
    "\n",
    "#### Criteria for Success:\n",
    "- The `get_word_stats()` method should return a frequency distribution of words as a `Counter` object, ensuring accurate word counting by normalizing to lowercase.\n",
    "- Sentences should be extracted cleanly from the pageâ€™s main text using the `split_into_sentences()` method.\n",
    "- The text should be properly cleaned and normalized (lowercased, and special characters removed) using the `clean_text()` method.\n",
    "\n",
    "#### Link: [How Very Logical](https://www.si.edu/newsdesk/snapshot/how-very-logical)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T13:45:36.493657Z",
     "start_time": "2024-09-16T13:45:36.488807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional, List, Dict\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "class SmithsonianTextAnalyzer:\n",
    "\tdef __init__(self, url: str) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the SmithsonianTextAnalyzer with a URL.\n",
    "\t\t:param url: The URL of the Smithsonian page to analyze.\n",
    "\t\t\"\"\"\n",
    "\t\tself.url: str = url\n",
    "\t\tself.text: str = \"\"  # Cleaned text content\n",
    "\t\tself.sentences: List[str] = []  # List of sentences\n",
    "\t\tself.word_frequency: Optional[Counter] = None  # Word frequency as Counter\n",
    "\n",
    "\tdef fetch_page(self) -> Optional[bytes]:\n",
    "\t\t\"\"\"\n",
    "\t\tFetch the HTML content of the Smithsonian Snapshot page.\n",
    "\t\t:return: The HTML content as bytes if successful, otherwise None.\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(self.url)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\t\t\treturn response.text\n",
    "\t\texcept requests.exceptions.RequestException:\n",
    "\t\t\treturn None\n",
    "\n",
    "\tdef clean_text(self, html_content: bytes) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tUse BeautifulSoup to extract clean text from the HTML content.\n",
    "\t\tRemove scripts, styles, and special characters.\n",
    "\t\t:param html_content: The HTML content as bytes.\n",
    "\t\t:return: None.\n",
    "\t\t\"\"\"\n",
    "\t\t# Note that the both sentences with and without punctuations are going to be both graded as correct answers.\n",
    "\t\t# steps to clean the text:\n",
    "\t\t# Remove scripts, styles, and unwanted tags\n",
    "\t\t# Remove extra spaces, and normalize text\n",
    "\t\t# Lowercase for normalization\n",
    "\t\tsoup = BeautifulSoup(html_content, 'html.parser')\n",
    "\t\ttext = soup.get_text(separator=\" \")\n",
    "\t\ttext = re.sub(r'\\s+', ' ', text)\n",
    "\t\ttext = text.strip().lower()\n",
    "\t\tself.text = text\n",
    "\n",
    "\tdef split_into_sentences(self) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tUse nltk's sentence tokenizer to split the cleaned text into sentences.\n",
    "\t\t:return: None. Updates the self.sentences attribute.\n",
    "\t\t\"\"\"\n",
    "\t\tdef clean(text):\n",
    "\t\t\ttext = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "\t\t\ttext = re.sub(r'\\s+', ' ', text)\n",
    "\t\t\ttext = text.lower().strip()\n",
    "\t\t\treturn text\n",
    "\t\t\n",
    "\t\tself.sentences = [clean(sent) for sent in nltk.sent_tokenize(self.text)]\n",
    "\n",
    "\tdef get_word_stats(self) -> Counter:\n",
    "\t\t\"\"\"\n",
    "\t\tCount the frequency of each word in the text. Return a Counter object.\n",
    "\t\tEnsure the text is lowercased for accurate counting.\n",
    "\t\t:return: A Counter object with word frequencies.\n",
    "\t\t\"\"\"\n",
    "\t\ttext = self.text.lower()\n",
    "\t\ttext = re.sub(r'\\W', ' ', text)\n",
    "\t\ttext = re.sub(r'\\s+', ' ', text)\n",
    "\t\t\n",
    "\t\twords = nltk.word_tokenize(text)\n",
    "\t\treturn Counter(words)\n",
    "\n",
    "\tdef analyze(self) -> Optional[Counter]:\n",
    "\t\t\"\"\"\n",
    "\t\tOrchestrate the fetching, cleaning, and analysis of the text from the page.\n",
    "\t\t- Fetch the HTML content.\n",
    "\t\t- Clean the text.\n",
    "\t\t- Split into sentences.\n",
    "\t\t- Get word frequency statistics.\n",
    "\t\t:return: A Counter object with word frequencies if successful, otherwise None.\n",
    "\t\t\"\"\"\n",
    "\t\tcontent = self.fetch_page()\n",
    "\t\tif content:\n",
    "\t\t\tself.clean_text(content)\n",
    "\t\t\tself.split_into_sentences()\n",
    "\t\t\tself.word_frequency = self.get_word_stats()\n",
    "\t\t\treturn self.word_frequency\n",
    "\t\treturn None"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Building a Smithsonian Snapshots Crawler\n",
    "\n",
    "#### 4.0.4. Task Description\n",
    "In this task, you will create a **web crawler** that will start at the Smithsonian Snapshots page and follow links to gather and analyze the content from multiple snapshot pages. The Smithsonian Snapshot section contains multiple articles, and your crawler will explore these articles, download their content, and process the information.\n",
    "\n",
    "You will implement a web crawler that:\n",
    "1. Starts at the [Smithsonian Snapshots Page](https://www.si.edu/snapshot).\n",
    "2. Crawls through snapshot pages, extracting key information (links, images, and text) from each page.\n",
    "3. Follows links from the initial page to other snapshot articles up to a specified depth.\n",
    "4. Processes and stores the content from each crawled page.\n",
    "\n",
    "### Tasks:\n",
    "1. **Implement a Crawler (`crawl(url: str, depth: int = 0) -> None`)**: Start crawling from the [Smithsonian Snapshots Page](https://www.si.edu/newsdesk/snapshots), gather links to snapshot articles, and visit each article. The method should be recursive, handling different crawl depths, and visiting pages only once.\n",
    "   \n",
    "2. **Content Extraction (`extract_content(html_content: bytes, base_url: str) -> Dict[str, Optional[str]]`)**: For each visited page, extract the following:\n",
    "   - Anchor tags (`'link_text', 'absolute_url'`).\n",
    "   - Image URLs (absolute URLs).\n",
    "   - Cleaned text content from the body of the article.\n",
    "\n",
    "3. **Depth Control (`max_depth: int`)**: Implement a parameter to control the depth of the crawl (i.e., how many levels of links the crawler should follow). Ensure that the crawler respects the specified depth and doesnâ€™t exceed it.\n",
    "\n",
    "4. **Yield Results (`crawl_generator() -> Generator[Dict[str, Optional[str]], None, None]`)**: Your crawler should return a **generator** that yields the results (text, links, images) as soon as a page is processed. This should allow the results to be streamed rather than collected all at once.\n",
    "\n",
    "### Criteria for Success:\n",
    "- The crawler should respect the specified depth and only crawl the specified number of levels.\n",
    "- Each snapshot page should have its content (links, images, text) extracted and returned in a structured format (i.e., as a dictionary).\n",
    "- The crawler should handle relative links and convert them to absolute URLs.\n",
    "- The content should be cleaned and stored properly, and the crawler should only revisit each URL once to avoid redundancy.\n",
    "- Ensure the total number of visited links doesn't exceed 10, as long runtimes may result in penalties.\n",
    "\n",
    "#### Link: [Smithsonian Snapshots Page](https://www.si.edu/snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T13:45:37.661413Z",
     "start_time": "2024-09-16T13:45:37.654029Z"
    }
   },
   "source": [
    "from typing import Optional, Dict, Generator\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class SmithsonianCrawler:\n",
    "\tdef __init__(self, start_url: str, max_depth: int = 2) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the SmithsonianCrawler with a start URL and a maximum crawl depth.\n",
    "\t\t:param start_url: The URL where the crawler begins.\n",
    "\t\t:param max_depth: The maximum depth the crawler will visit.\n",
    "\t\t\"\"\"\n",
    "\t\tself.start_url: str = start_url\n",
    "\t\tself.max_depth: int = max_depth\n",
    "\t\tself.visited: set = set()  # Set of visited URLs\n",
    "\n",
    "\tdef fetch_page(self, url: str) -> Optional[bytes]:\n",
    "\t\t\"\"\"\n",
    "\t\tFetch the HTML content from the given URL.\n",
    "\t\t:param url: The URL to fetch the page content from.\n",
    "\t\t:return: The page content as bytes if successful, otherwise None.\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(url)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\t\t\treturn response.text\n",
    "\t\texcept requests.exceptions.RequestException:\n",
    "\t\t\treturn None\n",
    "\n",
    "\tdef extract_content(self, html_content: bytes, base_url: str) -> Dict[str, Optional[str]]:\n",
    "\t\t\"\"\"\n",
    "\t\tExtract links, images, and clean text content from the page using BeautifulSoup.\n",
    "\t\tHandle relative URLs appropriately.\n",
    "\t\t:param html_content: The HTML content to parse.\n",
    "\t\t:param base_url: The base URL to resolve relative URLs.\n",
    "\t\t:return: A dictionary containing 'anchors', 'images', and 'text'.\n",
    "\t\t\"\"\"\n",
    "\t\tsoup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\t\t# Extract links\n",
    "\t\tlinks = []\n",
    "\t\tfor a in soup.find_all('a', href=True):\n",
    "\t\t\tabsolute_url = urljoin(base_url, a['href'])\n",
    "\t\t\tlinks.append((a.text, absolute_url))\n",
    "\n",
    "\t\t# Extract images\n",
    "\t\timages = []\n",
    "\t\tfor img in soup.find_all('img', src=True):\n",
    "\t\t\tabsolute_url = urljoin(base_url, img['src'])\n",
    "\t\t\timages.append(absolute_url)\n",
    "\n",
    "\t\t# Extract clean text content\n",
    "\t\ttext = soup.get_text()\n",
    "\t\tcleaned_text = ' '.join(text.split())\n",
    "\t\tcleaned_text = cleaned_text.strip()\n",
    "\t\t\n",
    "\t\treturn {\"anchors\": links, \"images\": images, \"text\": cleaned_text}\n",
    "\n",
    "\tdef crawl(self, url: str, depth: int = 0) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tRecursively crawl through pages starting from the given URL up to a specified depth.\n",
    "\t\tFollow links and process the page content.\n",
    "\t\t:param url: The URL to crawl.\n",
    "\t\t:param depth: The current depth of the crawl.\n",
    "\t\t:return: None.\n",
    "\t\t\"\"\"\n",
    "\t\tif depth > self.max_depth or url in self.visited:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tself.visited.add(url)\n",
    "\t\thtml_content = self.fetch_page(url)\n",
    "\t\tif html_content:\n",
    "\t\t\tdata = self.extract_content(html_content, url)\n",
    "\t\t\tyield data\n",
    "\n",
    "\t\t\t# Follow links and crawl recursively\n",
    "\t\t\tfor _, link in data['links']:\n",
    "\t\t\t\tyield from self.crawl(link, depth + 1)\n",
    "\n",
    "\n",
    "\tdef crawl_generator(self) -> Generator[Dict[str, Optional[str]], None, None]:\n",
    "\t\t\"\"\"\n",
    "\t\tA generator that yields the extracted content of each crawled page as soon as it's processed.\n",
    "\t\t:yield: A dictionary containing the extracted content from each page (anchors, images, text).\n",
    "\t\t\"\"\"\n",
    "\t\tyield from self.crawl(self.start_url)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
